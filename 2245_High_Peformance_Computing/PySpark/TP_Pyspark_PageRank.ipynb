{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_Pyspark_PageRank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY",
        "colab_type": "text"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "This installs Apache Spark 3.0.0, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0N3qjw0gUJd",
        "colab_type": "code",
        "outputId": "547ba79f-203d-4e23-9d18-a8494c9b207c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install pyspark\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz  \n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl",
        "colab_type": "text"
      },
      "source": [
        "# Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnb_ePUyQIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P",
        "colab_type": "text"
      },
      "source": [
        "# Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ULPx4Y1LiR",
        "colab_type": "text"
      },
      "source": [
        "# Use Spark!\n",
        "That's all there is to it - you're ready to use Spark!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJp8ZI-VzYEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzAfgnqjd1jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we will close this context for now\n",
        "spark.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cxQ-uK_UFMi",
        "colab_type": "text"
      },
      "source": [
        "# Word count in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kq2WpknYxJW"
      },
      "source": [
        "Let's start a simple pyspark example with an histogram of words computation. Your task here is to count the number of occurences of words in a simple python string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLQMheDgYyu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Wordcount\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYyepyxIY5eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string = [\"le petit chat est mort, le petit chat est mort ce soir.\\\n",
        "            Il était confiné dans son panier.\"]\n",
        "\n",
        "# first create the RDD from the string\n",
        "words = # todo\n",
        "\n",
        "# take care of removing the punctuation ! ',' or '.'\n",
        "\n",
        "# count the occurrence of each word\n",
        "wordCounts = # Todo\n",
        "print(wordCounts.collect())\t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "czd2UVw0YpuF",
        "colab": {}
      },
      "source": [
        "# shutdown the context\n",
        "sc.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSH_1pLyI3Dl",
        "colab_type": "text"
      },
      "source": [
        "# PageRank with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NhonCDeLVQI",
        "colab_type": "text"
      },
      "source": [
        "We use the [Zachary Karate Club dataset](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) for this part of the practical session. First we can inspect the nodes degrees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZkw_gPEQvId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.karate_club_graph() # load Karate Club graph\n",
        "pos = nx.spring_layout(G) # needed to always display the graph in the same way\n",
        "print(\"Node Degree\")\n",
        "for v in G:\n",
        "    print('%s %s' % (v, G.degree(v)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMgu4tj_K5fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def draw_graph(G,pos,ranks=[],with_labels=True):\n",
        "  if len(ranks) is not 0:\n",
        "    nx.draw(G, pos, node_color=ranks, cmap=plt.cm.Blues, \n",
        "            with_labels=with_labels)\n",
        "  else:\n",
        "    nx.draw(G, pos, with_labels=with_labels)\n",
        "  plt.show()\n",
        "\n",
        "draw_graph(G,pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCqM3QwNL5op",
        "colab_type": "text"
      },
      "source": [
        "Note that you can specify a ranks table of float values, that will be used to display the rankings computed by Page Rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQjYxIDI7DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranks = np.random.rand(G.number_of_nodes())\n",
        "draw_graph(G,pos,ranks=ranks,with_labels=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGOuzoYmdp1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"PageRank\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5mqMyraNewB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vertices=set(G) #list vertices\n",
        "verticeRDD =  #TODO create RDD of vertices\n",
        "neighRDD = #TODO create RDD of neighbors which associates one index of a node to its list of neighbors\n",
        "rankRDD =  #TODO create RDD of ranks which associates one index of a node to its rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN9o2x1njKlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getRanks(rankRDD):\n",
        "  tab=[]\n",
        "  for (link, rank) in rankRDD.collect():\n",
        "    tab.append(rank)\n",
        "  return tab\n",
        "\n",
        "# a simple example to undertstand what the above functions do\n",
        "test = rankRDD.collect()\n",
        "print(test)\n",
        "print(getRanks(rankRDD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATjnLSkbmiQN",
        "colab_type": "text"
      },
      "source": [
        "Now you are ready to implement the pagerank method in pyspark !\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Nxn6dSaeb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO add method if needed\n",
        "\n",
        "def pagerank(neighRDD,rankRDD,alpha=0.9,NUM_ITERATIONS = 50,display=True,verbose=True):\n",
        "  for i in range(NUM_ITERATIONS): # is there a better way to test for convergence rather than doing all the iterations ?\n",
        "    if verbose:\n",
        "      print(\"Iteration {}\".format(i))\n",
        "      print(getRanks(rankRDD))\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    if display:\n",
        "      draw_graph(G,pos,ranks=getRanks(rankRDD),with_labels=False)\n",
        "    return rankRDD\n",
        "\n",
        "result = pagerank(neighRDD,rankRDD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwkzAVLqmGea",
        "colab_type": "text"
      },
      "source": [
        "You can compare the result you obtained with the [pagerank](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html) method from Newtorkx\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y2IQnX7ghry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr = nx.pagerank(G, alpha=0.9)\n",
        "draw_graph(G,pos,ranks=list(pr.values()),with_labels=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIfGIyIXmdyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}