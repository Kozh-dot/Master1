{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification : Bayésien naïf, LDA, QDA, et $k$-Plus Proches Voisins\n",
    "\n",
    "L'objectif de cette séance de TD/TP est\n",
    "1. d'implémenter les différents algorithmes de classificaion vus en cours \n",
    " * bayésien naïf\n",
    " * LDA et QDA \n",
    " * $k$-PPV\n",
    "2. d'évaluer les performances de classification sur *les échantillons d'apprentissage*\n",
    " * calcul d'une matrice de confusion\n",
    " * calcul de l'Overall Accuracy et du coefficient Kappa\n",
    "3. de visualiser les frontières de décision obtenues par les algorithmes de classification\n",
    "\n",
    "L'ensemble des algorithmes à implémenter sont déterministiques (c'est-à-dire qu'ils ne dépendent pas d'un caractère aléatoire). Vos implémentations devraient donc théoriquement vous donner des résultats strictement identiques à ceux de Scikit-Learn. Cependant, la résolution de LDA par Scikit-Learn n'est pas exacte et l'algorithme des k-PPV peut utiliser de l'aléatoire pour départager les cas d'égalité lors de l'affectation de la classe majoritaire (diapo 32 du cours).\n",
    "\n",
    "**Indication** : le découpage en fonction (avec les résultats attendus) est là pour vous aider à modulariser votre code. Cependant, vous pouvez partir sur un tout autre choix d'implémentation (notamment d'autres paradigmes de programmation comme l'orienté objet ou le fonctionnel).\n",
    "\n",
    "\n",
    "## Jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- import\n",
    "from sklearn import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances #to compute pairwise distances\n",
    "from numpy.linalg import inv #to compute X^-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Charger le jeu de données iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,0:2] # on garde uniquement deux variables explicatives\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0,ddof=0)\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Scatterplot\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y) #plot the points with their label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur bayésien naïf\n",
    "\n",
    "### 1.  Algotihme de Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Trouver le nombre de classes\n",
    "C = len(np.unique(y))\n",
    "print(\"C =\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Bayésien naif gaussien\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X, y).predict(X)\n",
    "print (\"Taux d'erreur du classifieur bayésien naïf\", np.mean(y_pred != y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Figure (frontières de décision)\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "Z = gnb.fit(X, y).predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"Bayésien naïf gaussien (Scikit-learn)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Votre version de l'algorithme Bayésien naif\n",
    "\n",
    "Astuces : \n",
    "* `np.bincount(y)` permet de compter le nombre d'occurences dans une classe\n",
    "* `np.mean` et `np.var` permettent de calculer la moyenne et la variance pour une variable explicative\n",
    "* loi normale pour $z$ la variable aléatoire : $p(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2\\sigma^2}(z-\\mu)^2}$\n",
    "* `grid = np.c_[xx.ravel(),yy.ravel()]` défini une matrice avec toutes les positions sur une grille (cf. le code de la Figure ci-dessus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- probabilité a priori (Pr(Y=y))\n",
    "def proba_classe():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proba_classe(y)) #- fréquence d'apparition de chacune des classes dans les données d'apprentissage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "[0.33333333 0.33333333 0.33333333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_param_nb():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimation_param_nb(X,y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "(array([[-1.01457897,  0.85326268],\n",
    "       [ 0.11228223, -0.66143204],\n",
    "       [ 0.90229674, -0.19183064]]), array([[0.17876968, 0.74619175],\n",
    "       [0.38334383, 0.51135882],\n",
    "       [0.5817693 , 0.54010089]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- loi normale (1D)\n",
    "def loi_normale():\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loi_normale(X[:,0],mu=0,sigma2=1)[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "array([0.26592211, 0.20759165, 0.15281312, 0.12825426, 0.23668473,\n",
    "       0.34534257, 0.12825426, 0.23668473, 0.08645012, 0.20759165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- vraisemblance (Pr(X=x|Y=y)) en fonction des paramètres mu et sigma\n",
    "def vraisemblance_nb():    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma2 = estimation_param_nb(X,y)\n",
    "print(vraisemblance_nb(X,mu,sigma2)[:10,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "[[4.12568710e-01 5.96043661e-03 4.47104459e-03]\n",
    " [2.17134544e-01 3.49960316e-02 7.76844498e-03]\n",
    " [2.46660743e-01 7.39752504e-03 2.46053810e-03]\n",
    " [1.51142680e-01 6.70198548e-03 1.79316099e-03]\n",
    " [3.92250378e-01 1.89179325e-03 1.72350876e-03]\n",
    " [1.04438709e-01 2.77596442e-04 7.12783062e-04]\n",
    " [2.20840906e-01 1.50708277e-03 7.95813815e-04]\n",
    " [4.34485017e-01 8.58896773e-03 4.83801148e-03]\n",
    " [3.58456742e-02 3.59355840e-03 6.57792476e-04]\n",
    " [2.83993851e-01 2.61821854e-02 7.21025739e-03]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- classifieur bayésien naif (NB : Naif Bayesian classifier)\n",
    "def predict_nb(X,Xtrain,ytrain):\n",
    "    \"\"\"\n",
    "        Prédiction des étiquettes pour les échantillons X lorsque le modèle est appris sur les données (Xtrain,ytrain)\n",
    "        IN:\n",
    "            - X (N,d) : observations pour laquelle on souhaite prédire la classe yhat\n",
    "            - Xtrain (m,d) : données d'apprentissage\n",
    "            - ytrain (m,): étiqueetes associées aux données d'apprentissage\n",
    "        OUT:\n",
    "            - yhat (N,) : classes prédites pour les observations X \n",
    "    \"\"\"\n",
    "    mu, sigma2 = estimation_param_nb(Xtrain,ytrain)\n",
    "    yhat = np.argmax(proba_classe(ytrain) * vraisemblance_nb(X, mu, sigma2), axis=1)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_nb(X,X,y)) # prédictions sur les données d'apprentissage (donc X=Xtrain dans ce TP)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## RESULTAT ATTENDU\n",
    "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 1 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
    " 2 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 2 1 2 2 2 2\n",
    " 1 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 1 2 2 2 1 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
    " 2 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Calcul de la matrice de confusion\n",
    "def getConfusionMatrix():\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Calcul du taux de bonne classification (OA)\n",
    "def getOA():\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Calcul du coefficient Kappa\n",
    "def getKappa():\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- test\n",
    "yhat = predict_nb(X,X,y)\n",
    "print (\"Taux d'erreur du classifieur bayésien naïf\", 1-getOA(y,yhat))\n",
    "print (\"\\tet coefficient Kappa\", getKappa(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Taux d'erreur du classifieur bayésien naïf (Scikit-learn)\", 1-accuracy_score(y,yhat))\n",
    "print (\"\\tet coefficient Kappa (Scikit-learn)\", cohen_kappa_score(y,yhat))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "Taux d'erreur du classifieur bayésien naïf 0.21999999999999997\n",
    "    et coefficient Kappa 0.67\n",
    "\n",
    "Taux d'erreur du classifieur bayésien naïf (Scikit-learn) 0.21999999999999997\n",
    "    et coefficient Kappa (Scikit-learn) 0.6699999999999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualisation des frontières de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Figure (frontières de décision) \n",
    "## A COMPARER AVEC LE PLOT INITIAL\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "Z = predict_nb(np.c_[xx.ravel(), yy.ravel()],X,y)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"Bayésien naïf gaussien (Scikit-learn)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA et QDA\n",
    "\n",
    "### 1. Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- LDA \n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y) \n",
    "y_pred = lda.predict(X)\n",
    "print (\"Taux d'erreur LDA\", np.mean(y_pred != y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure (frontière de décision)\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),np.arange(y_min, y_max, 0.05))\n",
    "Z = lda.fit(X, y).predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"Linear Discriminant Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- QDA\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X, y) \n",
    "y_pred = qda.predict(X)\n",
    "print (\"Taux d'erreur QDA\", np.mean(y_pred != y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure (frontière de décision)\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "Z = qda.fit(X, y).predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"Quadratic Discriminant Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Votre version des algorithmes LDA et QDA\n",
    "\n",
    "Astuce :\n",
    "* ``np.cov`` calcul de covariance d'une numpy array (`bias=True`pour obtenir des résultats identiques à l'algorithme QDA de Scikit-Learn)\n",
    "* loi normale multivariée : $p(z) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(z-\\mu)^T \\Sigma^{-1}(z-\\mu)}$\n",
    "* ``np.linalg.det`` et ``np.linalg.inv`` calcul de déterminant et de l'inverse d'une numpy array\n",
    "* ``np.einsum`` (non obligatoire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance():\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Test\n",
    "cov = covariance(X)\n",
    "print(cov)\n",
    "print(cov.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "[[ 1.00671141 -0.11010327]\n",
    " [-0.11010327  1.00671141]]\n",
    "(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- loi normale multivariée\n",
    "def loi_normale_multivarie(X,mu,Sigma):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loi_normale_multivarie(X,[0, 0],[[1, 0], [0,1]])[:10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "[0.06312267 0.08209894 0.05776305 0.05091985 0.04327343 0.02099329\n",
    " 0.037486   0.06917792 0.03229923 0.08241859]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_param_lda(Xtrain, ytrain):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimation_param_lda(X,y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu (la valeur de Sigma peut différer en fonction du calcul de variance réalisé)\n",
    "(array([[-1.01457897,  0.85326268],\n",
    "       [ 0.11228223, -0.66143204],\n",
    "       [ 0.90229674, -0.19183064]]), array([[ 1.        , -0.11756978],\n",
    "       [-0.11756978,  1.        ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- vraisemblance (Pr(X=x|Y=y)) en fonction des paramètres mu et sigma\n",
    "def vraisemblance_lda(X, mu, Sigma):    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- LDA\n",
    "def predict_lda(X, Xtrain, ytrain):\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- test\n",
    "yhat = predict_lda(X,X,y)\n",
    "print (\"Taux d'erreur LDA\", 1-getOA(y,yhat))\n",
    "print (\"\\tet coefficient Kappa\", getKappa(y,yhat))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "Taux d'erreur LDA 0.21333333333333337\n",
    "    et coefficient Kappa 0.6799999999999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_param_qda(Xtrain, ytrain):\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- vraisemblance (Pr(X=x|Y=y)) en fonction des paramètres mu et sigma\n",
    "def vraisemblance_qda(X, mu, Sigma):    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- QDA\n",
    "def predict_qda(X, Xtrain, ytrain):\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- test\n",
    "yhat = predict_qda(X,X,y)\n",
    "print (\"Taux d'erreur QDA\", 1-getOA(y,yhat))\n",
    "print (\"\\tet coefficient Kappa\", getKappa(y,yhat))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "Taux d'erreur QDA 0.19999999999999996\n",
    "    et coefficient Kappa 0.7000000000000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : On remarque que OA(LDA) > OA(QDA), mais Kappa(LDA)<Kappa(QDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualisation des frontières de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure (frontière de décision)\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),np.arange(y_min, y_max, 0.05))\n",
    "Z = predict_lda(np.c_[xx.ravel(), yy.ravel()],X,y)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"Linear Discriminant Analysis\")\n",
    "plt.show()\n",
    "\n",
    "# Figure (frontière de décision)\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "Z = predict_qda(np.c_[xx.ravel(), yy.ravel()],X,y)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"Quadratic Discriminant Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Il est possible que le résultat visuel pour votre implémentation de LDA diffère de celle de Scikit-Learn vu précédemment. Cette différence est due à l'utilisation d'un algorithme d'optimisation pour l'apprentissage des paramètres du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Plus Proches Voisins\n",
    "\n",
    "### 1. Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- k-PPV\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X, y) \n",
    "y_pred = knn.predict(X)\n",
    "print (\"Taux d'erreur du 3-PPV\", np.mean(y_pred != y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "Z = knn.fit(X, y).predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"3-NN (scikit learn)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Votre version de l'algorithme des $k$-Plus Proches Voisins\n",
    "\n",
    "Astuces :\n",
    "* `euclidean_distances()` calculer les distances entre échantillons ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html))\n",
    "* `np.argsort()` donner les indices des points\n",
    "* `np.unique()` pour avoir les éléments uniques d'une liste (et leur compte `return_counts=True`)\n",
    "* `map(lambda i:y[i], tri.ravel())` donner les valeurs de y associées au i-ième indice, stocké dans la liste appelée tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Votre algorithme\n",
    "def kPPV(X, Xtrain, ytrain, k=5):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kPPV(np.array([[0.5, 1.3],[-3.5,0.4],[1.5,-0.4]]),X,y, k=5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "array([2, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- test\n",
    "for k in [1,3,5,10,15,30]:\n",
    "    yhat = kPPV(X,X,y,k=k)\n",
    "    print (\"Taux d'erreur \"+str(k)+\"-PPV\", 1-getOA(y,yhat))\n",
    "    print (\"\\tet coefficient Kappa\", getKappa(y,yhat))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Résultat attendu\n",
    "Taux d'erreur 1-PPV 0.06000000000000005\n",
    "    et coefficient Kappa 0.9099999999999999\n",
    "Taux d'erreur 3-PPV 0.15333333333333332\n",
    "    et coefficient Kappa 0.77\n",
    "Taux d'erreur 5-PPV 0.16000000000000003\n",
    "    et coefficient Kappa 0.7599999999999999\n",
    "Taux d'erreur 10-PPV 0.15333333333333332\n",
    "    et coefficient Kappa 0.77\n",
    "Taux d'erreur 15-PPV 0.18666666666666665\n",
    "    et coefficient Kappa 0.72\n",
    "Taux d'erreur 30-PPV 0.18000000000000005\n",
    "    et coefficient Kappa 0.7299999999999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note \\#1** : Le taux d'erreur du 1-PPV devrait être de 0 % sur les échantillons d'apprentissage (chaque donnée d'apprentissage est la plus proche d'elle-même). **Comment pouvez vous expliquer le taux d'erreur de 6 % observé ?**\n",
    "\n",
    "**Note \\#2** : Il est rare d'utiliser une implémentation naïve du $k$-PPV qui calcule une distance entre chaque paire d'échantillons (coûteuse en mémoire et en temps). Pour aller plus loin : K-d tree.\n",
    "\n",
    "**Note \\#3** : En plus de l'hyperparamètre $k$ à fixer, le choix de la distance est aussi un hyperparamètre. Dans cet exemple, nous utilisons la distance euclidienne, mais vous pouvez tester d'autres types de distance (par exemple Minkowski pour $p\\neq 2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualisation des frontières de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure\n",
    "x_min, x_max, y_min, y_max = np.min(X[:, 0]),np.max(X[:, 0]) ,  np.min(X[:, 1]),  np.max(X[:, 1])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "Z = kPPV(np.c_[xx.ravel(), yy.ravel()],X,y,k=5)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.title(\"5-PPV (scikit learn)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}